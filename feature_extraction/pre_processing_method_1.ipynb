{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "htvQVg-dFBES"
   },
   "source": [
    "##Importing CSV File for Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qQgWvsalyVvx"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('final_lematize_translated_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HDlLnYCKE6OU"
   },
   "source": [
    "##Run all cells before preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ojbGCeDdxyMb"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import json\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZRxNhQFEx7PH",
    "outputId": "0a35878d-7998-4dc1-bfb6-5ec7cc35b6ef"
   },
   "outputs": [],
   "source": [
    "!pip install emoji\n",
    "!pip install demoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q3UEshY6x8uj",
    "outputId": "7ce12956-d186-4efb-f379-689e54a3e401"
   },
   "outputs": [],
   "source": [
    "nltk.download([\n",
    "...     \"names\",\n",
    "...     \"stopwords\",\n",
    "...     \"state_union\",\n",
    "...     \"averaged_perceptron_tagger\",\n",
    "...     \"vader_lexicon\",\n",
    "...     \"punkt\",\n",
    "        \"wordnet\",\n",
    "... ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h7XOQVdoyAA9"
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from emoji import UNICODE_EMOJI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LU6EOsdUyDzN"
   },
   "source": [
    "#PreProcessing Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "auGdfiZMyDr7"
   },
   "source": [
    "\n",
    "###Convert gujarati to english\n",
    "###Convert all emojis to text\n",
    "###Convert all text to lower case\n",
    "###Remove all special characters or links or user tags\n",
    "###Remove all stopwords\n",
    "###Lemmatize words\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SOOY8VtPyH2W"
   },
   "outputs": [],
   "source": [
    "#Function to remove https and @ tags \n",
    "def remove_https_and_tags(data): \n",
    "\n",
    "  #Splitting data into words\n",
    "  data = data.split()\n",
    "\n",
    "  text = ''\n",
    "  for word in data:\n",
    "    \n",
    "    #If word is not matched add it to string \n",
    "    if 'http' in word:\n",
    "      # print(word)\n",
    "      continue\n",
    "    elif 'youtu' in word:\n",
    "      continue\n",
    "    else:\n",
    "      text+=(word + ' ')\n",
    "\n",
    "  #Removes extra spaces from starting and ending\n",
    "  text.strip()\n",
    "\n",
    "  return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pDuMoll1yO_C"
   },
   "outputs": [],
   "source": [
    "#Function to convert emoji to text \n",
    "import emoji \n",
    "\n",
    "# add space near your emoji\n",
    "def add_space(text):\n",
    "  result = ''\n",
    "  for word in emoji.get_emoji_regexp().split(text):\n",
    "    result+=word\n",
    "    result+=' '\n",
    "\n",
    "  return result\n",
    "\n",
    "def convert_emoji_to_text(data):\n",
    "\n",
    "  data = data.split()\n",
    "\n",
    "  #Before converting emojis to text , space is required between word and emoji if there is no space between them\n",
    "  text = ''\n",
    "  for word in data:\n",
    "    text+=(add_space(word) + ' ')\n",
    "   \n",
    "  #Removes extra spaces from starting and ending \n",
    "  text = text.strip()\n",
    "\n",
    "  #Now convert emoji to text\n",
    "  text = emoji.demojize(text, delimiters=(\"\", \"\")) \n",
    "   \n",
    "  return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j0ebShsSyPuB"
   },
   "outputs": [],
   "source": [
    "def remove_special_characters(data):\n",
    "\n",
    "   #Converting all emojis to respective text\n",
    "   text = convert_emoji_to_text(data)  \n",
    "\n",
    "   #Keeping only alpha-numeric and space\n",
    "   text = re.sub(\"[^(a-zA-Z \\_)]\", \"\", text)\n",
    "\n",
    "   #Removes extra spaces\n",
    "   text = re.sub(\" +\", \" \", text)\n",
    "\n",
    "   #Removing 'https links' and user tags '@*'\n",
    "   text = remove_https_and_tags(text)\n",
    "\n",
    "   #Removes spaces from start and end\n",
    "   text = text.strip()\n",
    "\n",
    "   #converting data to lower case\n",
    "   text = text.lower()\n",
    "\n",
    "   #Keeping only alpha-numeric and space\n",
    "   text = re.sub(\"[^(a-zA-Z \\_)]\", \"\", text)\n",
    "  \n",
    "   return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pSBv0dJjyT0I"
   },
   "outputs": [],
   "source": [
    "def remove_stopwords(data):\n",
    "  \n",
    "  lemmatizer = WordNetLemmatizer()\n",
    "  data = nltk.word_tokenize(data)\n",
    "  text = \"\"\n",
    "  for word in data:\n",
    "     if word not in set(stopwords.words('english')):\n",
    "       word = lemmatizer.lemmatize(word)\n",
    "       text+= (word + \" \")\n",
    "  text = text.strip()\n",
    "\n",
    "  return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Ka2KKH8yZRg"
   },
   "source": [
    "#Updating Data Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xFH3xgIszDXx"
   },
   "source": [
    "##Previous method of converting list item to dictionary just for information\n",
    "\n",
    " msg = ast.literal_eval(msg)\n",
    "      \n",
    "      #finding first key,value pair\n",
    "      dict_pairs = msg.items()\n",
    "      pairs_iterator = iter(dict_pairs)\n",
    "      first_pair = next(pairs_iterator)\n",
    "      \n",
    "      #key is first_pair[0]\n",
    "      #value is first_pair[1]\n",
    "      \n",
    "      if(len(first_pair[1])):\n",
    "        msg = first_pair[1][0]\n",
    "        msg = remove_special_characters(msg)\n",
    "        msg = remove_stopwords(msg)\n",
    "        \n",
    "        if(len(msg)):\n",
    "          dict1 = {}\n",
    "          list_msg = []\n",
    "          list_msg.append(msg)\n",
    "          dict1[first_pair[0]] = list_msg\n",
    "          subcomment_data.append(dict1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mTcoGDrAzVXg"
   },
   "source": [
    "##Main Code\n",
    "\n",
    "###Same task are repeated for message,comment,subcomment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TUUbrt2-Dga9"
   },
   "source": [
    "##Preprocessing on CSV Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "56V2wnCVyrjx",
    "outputId": "7bdf1c1d-5983-4ccf-8ef4-82b713bdde2b"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import ast\n",
    "for i in range(data.shape[0]):\n",
    "# for i in range(4):\n",
    "\n",
    "  print(i)\n",
    "  #Message Section\n",
    "  message = data['post_msg_final'][i]\n",
    "  \n",
    "  #Making commas in text as $, so that list can be easily split \n",
    "  j=1\n",
    "  ans=\"[\"\n",
    "  while(j<len(message)-1):\n",
    "     if(message[j]=='}' and message[j+1]==','):\n",
    "       ans+=\"},\"\n",
    "       j+=2\n",
    "       continue\n",
    "     if(message[j]==','):\n",
    "       ans+='$'\n",
    "     else:\n",
    "       ans+=message[j]\n",
    "     j+=1\n",
    "  ans+=\"]\"\n",
    "  message = ans\n",
    "  message = message.strip('][').split(\", \")\n",
    "\n",
    "  message_data = []\n",
    "\n",
    "  #traversing list\n",
    "  for msg in message:    \n",
    "   \n",
    "    if(len(msg)):\n",
    "\n",
    "      ans = msg.strip('[{}]')\n",
    "      \n",
    "      #Breaking string into 2 parts id, message\n",
    "      if ':' and '[' in ans:\n",
    "        ans = ans.split(': [')\n",
    "      if ':' in ans:\n",
    "        ans = ans.split(':')\n",
    "      if '[' in ans:    #this part isn't working \n",
    "        ans = ans.split('[')\n",
    "\n",
    "      #ans[0] - id , ans[1] - message\n",
    "\n",
    "      #Remving whitespaces from beginning and end from id\n",
    "    #   print(ans[0])\n",
    "      if ans!=\"VALUE\":\n",
    "        if ans[0][1]==' ':\n",
    "          ans[0] = ans[0][:1]+ans[0][2:]\n",
    "        if ans[0][-2]==' ':\n",
    "          ans[0] = ans[0][:-2]+ans[0][-1]\n",
    "        if(ans[0][18]!='_'):\n",
    "          ans[0] = ans[0][:18] + '_'+ ans[0][18:]\n",
    "    \n",
    "      #If message exists\n",
    "      if(1<len(ans) and len(ans[1])):\n",
    "        msg = ans[1]\n",
    "        msg = remove_special_characters(msg)\n",
    "        msg = remove_stopwords(msg)\n",
    "\n",
    "        if(len(msg)):\n",
    "          dict1 = {}\n",
    "          list_msg = []\n",
    "          list_msg.append(msg)\n",
    "          dict1[ans[0]] = list_msg\n",
    "          message_data.append(dict1)\n",
    "  \n",
    "  # print(message_data)\n",
    "  data['post_msg_tranlsated_final'][i] = message_data\n",
    "\n",
    "\n",
    "  #Comments Section\n",
    "  comments = data['post_comments_final'][i]\n",
    "\n",
    "  #Modifying string where there is space btw { and ,\n",
    "  ans1=\"[\"\n",
    "  j=1\n",
    "  while(j<len(comments)-2):\n",
    "      if(comments[j]=='}' and comments[j+1]==' ' and comments[j+2]==','):\n",
    "         ans1+=\"},\"\n",
    "         j+=3\n",
    "         continue\n",
    "      else:\n",
    "        ans1+=comments[j]\n",
    "      j+=1\n",
    "  ans1+=comments[len(comments)-2]\n",
    "  ans1+=\"]\"\n",
    "  comments = ans1\n",
    "\n",
    "  #Making commas in text as $, so that list can be easily split \n",
    "  ans=\"[\"\n",
    "  j=1\n",
    "  while(j<len(comments)-1):\n",
    "     if(comments[j]=='}' and comments[j+1]==','):\n",
    "       ans+=\"},\"\n",
    "       j+=2\n",
    "       continue\n",
    "     if(comments[j]==','):\n",
    "       ans+='$'\n",
    "     else:\n",
    "       ans+=comments[j]\n",
    "     j+=1\n",
    "  ans+=\"]\"\n",
    "  comments=ans\n",
    "  comments = comments.strip('][').split(', ')\n",
    "\n",
    "  #traversing list\n",
    "  comment_data = []\n",
    "  for msg in comments:\n",
    "    \n",
    "    if(len(msg)):\n",
    "      \n",
    "      ans = msg.strip('[{}]')\n",
    "\n",
    "\n",
    "      if ':' and '[' in ans:\n",
    "        ans = ans.split(': [')\n",
    "      elif ':' in ans:\n",
    "        ans = ans.split(':')\n",
    "      elif '[' in ans:\n",
    "        ans = ans.split('[')\n",
    "      \n",
    "    #   print(ans)      #    ---------\n",
    "      if ans!=\"VALUE\" and len(ans[0]) > 1:\n",
    "        if ans[0][1]==' ':\n",
    "          ans[0] = ans[0][:1]+ans[0][2:]\n",
    "        if ans[0][-2]==' ':\n",
    "          ans[0] = ans[0][:-2]+ans[0][-1]\n",
    "    \n",
    "      if(1<len(ans) and len(ans[1])):\n",
    "\n",
    "        msg = ans[1]\n",
    "        msg = remove_special_characters(msg)\n",
    "        msg = remove_stopwords(msg)\n",
    "\n",
    "        if(len(msg)):\n",
    "          dict1 = {}\n",
    "          list_msg = []\n",
    "          list_msg.append(msg)\n",
    "          dict1[ans[0]]=list_msg\n",
    "          comment_data.append(dict1)\n",
    "\n",
    "\n",
    "  # print(comment_data)\n",
    "  data['post_comments_translated_final'][i] = comment_data\n",
    "\n",
    "\n",
    "\n",
    "  #SubComments Section\n",
    "  subcomments = data['post_subcomments_final'][i]\n",
    "  ans1=\"[\"\n",
    "  j=1\n",
    "  while(j<len(subcomments)-2):\n",
    "      if(subcomments[j]=='}' and subcomments[j+1]==' ' and subcomments[j+2]==','):\n",
    "         ans1+=\"},\"\n",
    "         j+=3\n",
    "         continue\n",
    "      else:\n",
    "        ans1+=subcomments[j]\n",
    "      j+=1\n",
    "  ans1+=subcomments[len(subcomments)-2]\n",
    "  ans1+=\"]\"\n",
    "  subcomments = ans1\n",
    "  j=1\n",
    "  ans=\"[\"\n",
    "  while(j<len(subcomments)-1):\n",
    "     if(subcomments[j]=='}' and subcomments[j+1]==','):\n",
    "       ans+=\"},\"\n",
    "       j+=2\n",
    "       continue\n",
    "     if(subcomments[j]==','):\n",
    "       ans+='$'\n",
    "     else:\n",
    "       ans+=subcomments[j]\n",
    "     j+=1\n",
    "  ans+=\"]\"\n",
    "  subcomments = ans\n",
    "  subcomments = subcomments.strip('][').split(', ')\n",
    "   \n",
    "  subcomment_data = []\n",
    "  for msg in subcomments:\n",
    "\n",
    "    if(len(msg)):\n",
    "\n",
    "      ans = msg.strip('[{}]')\n",
    "      \n",
    "      if ':' and '[' in ans:\n",
    "        ans = ans.split(': [')\n",
    "      if ':' in ans:\n",
    "        ans = ans.split(':')\n",
    "      if '[' in ans:\n",
    "        ans = ans.split('[')\n",
    "\n",
    "      if ans!=\"VALUE\" and len(ans[0]) > 1:\n",
    "        if ans[0][1]==' ':\n",
    "          ans[0] = ans[0][:1]+ans[0][2:]\n",
    "        if ans[0][-2]==' ':\n",
    "          ans[0] = ans[0][:-2]+ans[0][-1]\n",
    "\n",
    "      if(1<len(ans) and len(ans[1])):\n",
    "        msg = ans[1]\n",
    "        msg = remove_special_characters(msg)\n",
    "        msg = remove_stopwords(msg)\n",
    "        \n",
    "        if(len(msg)):\n",
    "          dict1 = {}\n",
    "          list_msg = []\n",
    "          list_msg.append(msg)\n",
    "          dict1[ans[0]] = list_msg\n",
    "          subcomment_data.append(dict1)\n",
    "\n",
    "  # print(subcomment_data) \n",
    "  data['post_subcomments_translated_final'][i] = subcomment_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YwlLuL0uDqks"
   },
   "source": [
    "###Exporting pre-processed data to new file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xatPht9TpW11"
   },
   "outputs": [],
   "source": [
    "data.to_csv('final_lematize_translated_data.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GqFfOAahDxQ9"
   },
   "source": [
    "##Reading preprocessed data columns from CSV File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2Pd9-q9h7dsK",
    "outputId": "5335aa52-f88e-4a9c-dd7d-f52b3146d9a6"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import ast\n",
    "\n",
    "complete_msg= []\n",
    "\n",
    "for i in range(data.shape[0]):\n",
    "\n",
    "  print(i)\n",
    " \n",
    "  #Message Section\n",
    "  message = data['post_msg_tranlsated_final'][i]\n",
    "  message = message.strip('][').split(\", \")\n",
    "\n",
    "  #traversing list\n",
    "  for msg in message:    \n",
    "   \n",
    "    if(len(msg)):\n",
    "      \n",
    "      msg = ast.literal_eval(msg)      \n",
    "\n",
    "      #finding first key,value pair\n",
    "      dict_pairs = msg.items()\n",
    "      pairs_iterator = iter(dict_pairs)\n",
    "      first_pair = next(pairs_iterator)\n",
    "    \n",
    "      #key is first_pair[0]\n",
    "      #value is first_pair[1]\n",
    "    \n",
    "      if(len(dict_pairs) and len(first_pair[1])):\n",
    "        msg = first_pair[1][0]\n",
    "        complete_msg.append(msg)\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "  #Comments Section\n",
    "  comments = data['post_comments_translated_final'][i]\n",
    "  comments = comments.strip('][').split(\", \")\n",
    "\n",
    "  #traversing list\n",
    "  for msg in comments:\n",
    "    \n",
    "    if(len(msg)):\n",
    "\n",
    "      msg = ast.literal_eval(msg)  \n",
    "      #finding first key,value pair\n",
    "      \n",
    "      dict_pairs = msg.items()\n",
    "      pairs_iterator = iter(dict_pairs)\n",
    "      first_pair = next(pairs_iterator)\n",
    "    \n",
    "      #key is first_pair[0]\n",
    "      #value is first_pair[1]\n",
    "    \n",
    "      if(len(dict_pairs) and len(first_pair[1])):\n",
    "        msg = first_pair[1][0]\n",
    "        complete_msg.append(msg)\n",
    "\n",
    "\n",
    "\n",
    "  #SubComments Section\n",
    "  subcomments = data['post_subcomments_translated_final'][i]\n",
    "  subcomments = subcomments.strip('][').split(\", \")\n",
    "\n",
    "  for msg in subcomments:\n",
    "\n",
    "    if(len(msg)):\n",
    "\n",
    "      msg = ast.literal_eval(msg)  \n",
    "\n",
    "      #finding first key,value pair\n",
    "      dict_pairs = msg.items()\n",
    "      pairs_iterator = iter(dict_pairs)\n",
    "      first_pair = next(pairs_iterator)\n",
    "    \n",
    "      #key is first_pair[0]\n",
    "      #value is first_pair[1]\n",
    "    \n",
    "      if(len(dict_pairs) and len(first_pair[1])):\n",
    "        msg = first_pair[1][0]\n",
    "        complete_msg.append(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XeIClj9wtrm0",
    "outputId": "aceed23f-ecb0-48cd-e39d-68fc84c8af4d"
   },
   "outputs": [],
   "source": [
    "sentences = complete_msg\n",
    "for i in range(len(sentences)):\n",
    "  print(i)\n",
    "  sentences[i] = nltk.word_tokenize(sentences[i])\n",
    "# print(sentences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gJmEqiXhD7gj"
   },
   "source": [
    "##Creating Word2vec model for given sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U8ZN7tlpFMlW",
    "outputId": "1737b5c5-4469-4e0d-eff7-cc7b81733dcc"
   },
   "outputs": [],
   "source": [
    "model = Word2Vec(sentences,min_count=2)\n",
    "words = model.wv.vocab\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lM9zh2azEJQ7"
   },
   "source": [
    "##Code for creating and uploading dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6x23Ojv5GTpi"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data1 = pd.read_csv('word_dictionary.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KXCwbL1QFfH3"
   },
   "outputs": [],
   "source": [
    "word_dict = []\n",
    "for key,item in words.items():\n",
    "  temp = []\n",
    "  temp.append(key)\n",
    "  temp.append(item)\n",
    "  word_dict.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CmwLZIVmMPXY"
   },
   "outputs": [],
   "source": [
    "colNames = data1.columns\n",
    "\n",
    "data1_new = pd.DataFrame(data=word_dict, columns=colNames)\n",
    "\n",
    "# concatenate old and new\n",
    "df_complete = pd.concat([data1, data1_new], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tSNIFM6LMpZp"
   },
   "outputs": [],
   "source": [
    "df_complete.to_csv('word_dictionary.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gSVPnGKSESsg"
   },
   "source": [
    "##Code for creating and updating vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JpogEmw_ESqF"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GAjj2bHMOCVY"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data2 = pd.read_csv('word_vector.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ewBYXpcdNP5a"
   },
   "outputs": [],
   "source": [
    "word_vec = []\n",
    "for key,item in words.items():\n",
    "  temp = []\n",
    "  temp.append(key)\n",
    "  temp.append(model.wv[key])\n",
    "  word_vec.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9z7111pQOOkC"
   },
   "outputs": [],
   "source": [
    "colNames = data2.columns\n",
    "\n",
    "data2_new = pd.DataFrame(data=word_vec, columns=colNames)\n",
    "\n",
    "# concatenate old and new\n",
    "df_complete1 = pd.concat([data2, data2_new], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mCQKJ7_xObJD"
   },
   "outputs": [],
   "source": [
    "df_complete1.to_csv('word_vector.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ymw_o-nLEahL"
   },
   "source": [
    "##Preprocessing data from text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e3_uWsxQhMIu",
    "outputId": "782b94cd-a5f8-41a0-f2e5-b5122c48b429"
   },
   "outputs": [],
   "source": [
    "file1 = open('data.txt', 'r')\n",
    "# file2 = open('lemmatized_data.txt','w')\n",
    "Lines = file1.readlines()\n",
    "\n",
    "i=0\n",
    "complete_data = []\n",
    "for msg in Lines:\n",
    "  print(i)\n",
    "  msg = remove_special_characters(msg)\n",
    "  msg = remove_stopwords(msg)\n",
    "  complete_data.append(msg)\n",
    "  i+=1\n",
    "\n",
    "file1.close()\n",
    "# file2.close()\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ptzEbRkpEk5-"
   },
   "source": [
    "##Tokenising sentence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pMzaP50DlsKk",
    "outputId": "1a5d4bee-3c12-4a66-f630-ac2f6c78dc3e"
   },
   "outputs": [],
   "source": [
    "sentences = complete_data\n",
    "for i in range(len(sentences)):\n",
    "  print(i)\n",
    "  sentences[i] = nltk.word_tokenize(sentences[i])\n",
    "  for word in sentences[i]:\n",
    "    if len(word)<=3:\n",
    "      sentences[i].remove(word)\n",
    "# print(sentences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LH0YXNh3ExYd"
   },
   "source": [
    "##Creating Word2vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wo3f5xKEmFei",
    "outputId": "704dece8-a436-4829-cd3b-945dfb291c6b"
   },
   "outputs": [],
   "source": [
    "model = Word2Vec(sentences,min_count=1)\n",
    "words = model.wv.vocab\n",
    "print(words)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "updated_data_clean_shared.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
