{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NfYuZDq5ujP8",
    "outputId": "e0f39e3b-4f7b-402d-94e6-43d858eca613"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L1njQIeUtM0h",
    "outputId": "ccec2977-2464-4049-bf2c-c281f2749088"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import io\n",
    "import unicodedata\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "from numpy import linalg\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "from nltk.corpus import webtext\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "  \n",
    "with open('SentenceScoredLem.txt', mode = 'x' ) as sst:\n",
    "    with open('mergeData.txt') as f:\n",
    "        lines = f.readlines()\n",
    "        # text = \"Congratulations $ Congratulations! $ Congratulations $ A rit no msg avyo 6 $\"\n",
    "        lcount = 0\n",
    "        for text in lines:\n",
    "            TXT = text\n",
    "            lcount += 1\n",
    "            # print(text)\n",
    "            # sent_tokenizer = PunktSentenceTokenizer(text)\n",
    "            # sents = sent_tokenizer.tokenize(text)\n",
    "            \n",
    "            # print(word_tokenize(text))\n",
    "            # print(sent_tokenize(text))\n",
    "            \n",
    "            # porter_stemmer = PorterStemmer()\n",
    "            \n",
    "            # nltk_tokens = nltk.word_tokenize(text)\n",
    "            \n",
    "            # for w in nltk_tokens:\n",
    "            #     print (\"Actual: % s Stem: % s\" % (w, porter_stemmer.stem(w)))\n",
    "                \n",
    "            \n",
    "            wordnet_lemmatizer = WordNetLemmatizer()\n",
    "            nltk_tokens = nltk.word_tokenize(text)\n",
    "            \n",
    "            lemmatized_nltk_tokens = []\n",
    "            for w in nltk_tokens:\n",
    "                # print (\"Actual: % s Lemma: % s\" % (w, wordnet_lemmatizer.lemmatize(w)))\n",
    "                lemmatized_nltk_tokens.append(wordnet_lemmatizer.lemmatize(w))\n",
    "\n",
    "\n",
    "            # text = nltk.word_tokenize(text)\n",
    "            # print(nltk.pos_tag(text))\n",
    "            lem_sent = \"\"\n",
    "            for w in lemmatized_nltk_tokens:\n",
    "                lem_sent += \" \" + w  \n",
    "            \n",
    "            sid = SentimentIntensityAnalyzer() \n",
    "            # tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "            \n",
    "            text = TXT\n",
    "            scores = sid.polarity_scores(lem_sent)\n",
    "            for key in sorted(scores):\n",
    "                sst.write('{0}, '.format(scores[key]))\n",
    "\n",
    "            sst.write(\"\\n\")\n",
    "            if lcount % 1000 == 0: print(lcount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2EWLH3fJ-4CB",
    "outputId": "e6910575-3469-4ff7-b28d-dccb9d69ce98"
   },
   "outputs": [],
   "source": [
    "!pip install afinn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YFillsWW_d06",
    "outputId": "40f2d9fa-64cf-4982-fc98-649b19e63eef"
   },
   "outputs": [],
   "source": [
    "#importing necessary libraries\n",
    "from afinn import Afinn\n",
    "import pandas as pd\n",
    "  \n",
    "#instantiate afinn\n",
    "afn = Afinn()\n",
    "  \n",
    "#creating list sentences\n",
    "# news_df = ['Congratulations $ Congratulations! $ Congratulations $ A rit no msg avyo 6 $e']\n",
    "           \n",
    "# # compute scores (polarity) and labels\n",
    "# scores = [afn.score(article) for article in news_df]\n",
    "# sentiment = ['positive' if score > 0 \n",
    "#                           else 'negative' if score < 0 \n",
    "#                               else 'neutral' \n",
    "#                                   for score in scores]\n",
    "      \n",
    "# afn.score('Congratulations $ Congratulations! $ Congratulations $ A rit no msg avyo 6 $e ')\n",
    "with open('SentenceScoredAFNLem.txt', mode = 'x' ) as ast:\n",
    "    with open('mergeData.txt') as f:\n",
    "        lines = f.readlines()\n",
    "        # text = \"Congratulations $ Congratulations! $ Congratulations $ A rit no msg avyo 6 $\"\n",
    "        lcount = 0\n",
    "        for text in lines:\n",
    "            TXT = text\n",
    "            lcount += 1\n",
    "            # print(text)\n",
    "            # sent_tokenizer = PunktSentenceTokenizer(text)\n",
    "            # sents = sent_tokenizer.tokenize(text)\n",
    "            \n",
    "            # print(word_tokenize(text))\n",
    "            # print(sent_tokenize(text))\n",
    "            \n",
    "            # porter_stemmer = PorterStemmer()\n",
    "            \n",
    "            # nltk_tokens = nltk.word_tokenize(text)\n",
    "            \n",
    "            # for w in nltk_tokens:\n",
    "            #     print (\"Actual: % s Stem: % s\" % (w, porter_stemmer.stem(w)))\n",
    "                \n",
    "            \n",
    "            wordnet_lemmatizer = WordNetLemmatizer()\n",
    "            nltk_tokens = nltk.word_tokenize(text)\n",
    "            \n",
    "            lemmatized_nltk_tokens = []\n",
    "            for w in nltk_tokens:\n",
    "                # print (\"Actual: % s Lemma: % s\" % (w, wordnet_lemmatizer.lemmatize(w)))\n",
    "                lemmatized_nltk_tokens.append(wordnet_lemmatizer.lemmatize(w))\n",
    "\n",
    "\n",
    "            # text = nltk.word_tokenize(text)\n",
    "            # print(nltk.pos_tag(text))\n",
    "            lem_sent = \"\"\n",
    "            for w in lemmatized_nltk_tokens:\n",
    "                lem_sent += \" \" + w  \n",
    "            \n",
    "            sid = SentimentIntensityAnalyzer() \n",
    "            # tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "            \n",
    "            text = TXT\n",
    "            # scores = sid.polarity_scores(lem_sent)\n",
    "            score = afn.score(lem_sent)\n",
    "            ast.write(str(score))\n",
    "\n",
    "            ast.write(\"\\n\")\n",
    "            if lcount % 1000 == 0: print(lcount)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "LemmatizedSemanticSentenceWithoutStemming.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
