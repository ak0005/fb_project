{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kv5aIEyPb2QD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f383e48a-991f-46fe-ec24-0608135fadba"
      },
      "source": [
        "#bring data .csv in working directory"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bDLa0dfQb53K",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "outputId": "1bad44e0-516f-40af-bcf3-e01d54df1a12"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "from sklearn import preprocessing\n",
        "import dateutil.parser as dateparser\n",
        "\n",
        "from scipy import stats\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "md3Bijv-cO9b"
      },
      "source": [
        "modelDir='model'\n",
        "inFile='data.csv'\n",
        "dataDir='./dataset'\n",
        "\n",
        "a='2021-02-11T11:59:32.586367'\n",
        "cutoff = dateparser.parse(a).timestamp()\n",
        "\n",
        "testRatio=0.2\n",
        "validationRatio=0.2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zq-9Dkijs1mQ"
      },
      "source": [
        "if not os.path.isdir(dataDir):\n",
        "  os.mkdir(dataDir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tjMm-ZgEcVSB"
      },
      "source": [
        "missing = []\n",
        "total=0\n",
        "\n",
        "with open(inFile,'r') as fp:\n",
        "  fp.readline()\n",
        "  lineNo=-1\n",
        "\n",
        "  for l in fp:\n",
        "    lineNo+=1\n",
        "    arr=l[:-1].split(',')\n",
        "    arr=arr[1:]\n",
        "    if len(arr)==0:\n",
        "      continue\n",
        "\n",
        "    total+=1\n",
        "    for k in range(len(arr)):\n",
        "      if len(arr[k])==0:\n",
        "        missing.append(k)\n",
        "\n",
        "print(len(missing),np.unique(missing),total)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kaPeW5LgdiiU"
      },
      "source": [
        "def createToDropTable():\n",
        "  toDrop=[]\n",
        "  timeGap=[]\n",
        "\n",
        "  with open(inFile,'r') as fp:\n",
        "    fp.readline()\n",
        "    lineNo=-1\n",
        "\n",
        "    for l in fp:\n",
        "      lineNo+=1\n",
        "      arr=l[:-1].split(',')\n",
        "      arr=arr[1:]\n",
        "      if len(arr)==0:\n",
        "        continue\n",
        "      \n",
        "      if len(arr[3])==0 or len(arr[4])==0:\n",
        "        toDrop.append(lineNo)\n",
        "      elif float(arr[4])-float(arr[3]) < 0:\n",
        "        toDrop.append(lineNo)\n",
        "      else:\n",
        "        timeGap.append(float(arr[4])-float(arr[3]))\n",
        "  \n",
        "  finalCutoff=cutoff - np.median(timeGap)\n",
        "  print(np.min(timeGap),np.max(timeGap),np.median(timeGap))\n",
        "\n",
        "  with open(inFile,'r') as fp:\n",
        "    fp.readline()\n",
        "    lineNo=-1\n",
        "\n",
        "    for l in fp:\n",
        "      lineNo+=1\n",
        "      arr=l[:-1].split(',')\n",
        "      arr=arr[1:]\n",
        "      if len(arr)==0:\n",
        "        continue\n",
        "      \n",
        "      if len(arr[3])==0 or len(arr[4])==0:\n",
        "        pass\n",
        "      elif float(arr[3]) > finalCutoff:\n",
        "        toDrop.append(lineNo)\n",
        "  \n",
        "  return toDrop\n",
        "\n",
        "toDrop = createToDropTable()\n",
        "print(total,len(toDrop),total-len(toDrop))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ct-PRAZWeiqA"
      },
      "source": [
        "### creating dictionaries \n",
        "dictToDrop={}\n",
        "for i in toDrop:\n",
        "  dictToDrop[i]=1\n",
        "\n",
        "dictForMedian={}\n",
        "uniqueMissing=np.unique(missing)\n",
        "for k in uniqueMissing:\n",
        "  dictForMedian[k]=[]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-rbCBjFzl_nV"
      },
      "source": [
        "def getMedian():\n",
        "  with open(inFile,'r') as fp:\n",
        "    fp.readline()\n",
        "    lineNo=-1\n",
        "\n",
        "    for l in fp:\n",
        "      lineNo+=1\n",
        "      arr=l[:-1].split(',')\n",
        "      arr=arr[1:]\n",
        "      if len(arr)==0 or lineNo in dictToDrop:\n",
        "        continue\n",
        "      \n",
        "      for k in uniqueMissing:\n",
        "        if (len(arr[k])):\n",
        "          dictForMedian[k].append(float(arr[k]))\n",
        "\n",
        "  for k in uniqueMissing:\n",
        "    dictForMedian[k]=np.median(dictForMedian[k])\n",
        "\n",
        "getMedian()\n",
        "print(dictForMedian)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cOc6JeVDn4UJ"
      },
      "source": [
        "size=total-len(toDrop)\n",
        "arr=np.arange(size)\n",
        "testSize=(int)(size*testRatio)\n",
        "validationSize=(int)((size-testSize)*validationRatio)\n",
        "trainSize=size -testSize - validationSize"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QYWSLQHYviOQ"
      },
      "source": [
        "np.random.shuffle(arr)\n",
        "testArr=arr[:testSize]\n",
        "validationArr=arr[testSize:testSize+validationSize]\n",
        "trainArr=arr[testSize+validationSize:]\n",
        "\n",
        "testArr=np.sort(testArr)\n",
        "trainArr=np.sort(trainArr)\n",
        "validationArr=np.sort(validationArr)\n",
        "\n",
        "print(len(testArr),len(trainArr),len(validationArr))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8d616lXUwrvQ"
      },
      "source": [
        "def addFile(fileName,arrOfIndex):\n",
        "  fileName=os.path.join(dataDir,fileName)\n",
        "\n",
        "  effectiveRow=0\n",
        "  arrIndex=0\n",
        "\n",
        "  data=[]\n",
        "\n",
        "  with open(inFile,'r') as fp:\n",
        "    fp.readline()\n",
        "    lineNo=-1\n",
        "\n",
        "    for l in fp:\n",
        "      if arrIndex >= len(arrOfIndex):\n",
        "        break\n",
        "\n",
        "      lineNo+=1\n",
        "      arr=l[:-1].split(',')\n",
        "      arr=arr[1:]\n",
        "      if len(arr)==0 or lineNo in dictToDrop:\n",
        "        continue\n",
        "      \n",
        "      #print(effectiveRow,arrOfIndex[arrIndex])\n",
        "      if effectiveRow != arrOfIndex[arrIndex]:\n",
        "        effectiveRow+=1\n",
        "        continue\n",
        "\n",
        "      effectiveRow+=1\n",
        "      arrIndex+=1\n",
        "      \n",
        "      vec=[]\n",
        "      for k in range(len(arr)):\n",
        "        val=0\n",
        "        if len(arr[k]) == 0:\n",
        "          val = dictForMedian[k]\n",
        "        else:\n",
        "          val = float(arr[k]) \n",
        "        vec.append(val)\n",
        "      \n",
        "      # print(effectiveRow,len(vec),'================================')\n",
        "      data.append(np.array(vec))\n",
        "\n",
        "  data=np.array(data,dtype='float64')\n",
        "  np.random.shuffle(data)\n",
        "  np.save(fileName+'.npy',data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FBMzRm3N0WIv"
      },
      "source": [
        "addFile('test',testArr)\n",
        "addFile('val',validationArr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2VXfrj7d0fol"
      },
      "source": [
        "batch_size=3000\n",
        "step=(int)(trainSize/batch_size)\n",
        "\n",
        "for i in range(step):\n",
        "  addFile('batch_'+str(i),trainArr[i*batch_size : (i+1)*batch_size])\n",
        "\n",
        "if batch_size*step != trainSize:\n",
        "  addFile('batch_'+str(step),trainArr[step*batch_size : ])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gb_9uc8C1iJw"
      },
      "source": [
        "#zip -r -9 dataset.zip ./dataset\n",
        "#!cp -r ./dataset.zip /content/drive/MyDrive/fb-project/dataset.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CP_2CnQy10eE"
      },
      "source": [
        "**Neural Network**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ah0hfCa8VidH"
      },
      "source": [
        "!cp -r  /content/drive/MyDrive/fb-project/dataset.zip ./"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mFT0y_4aVnWP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4d0f5f2-0823-460f-c6e1-c7ead4586ebb"
      },
      "source": [
        "!unzip dataset.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xx3tl0b8VXR5"
      },
      "source": [
        "import numpy as np\n",
        "modelDir='model'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "spvw7F8j6o-p",
        "outputId": "7f7e4fa8-01b6-4ef0-bace-59cc76224736"
      },
      "source": [
        "b0=np.load('batch_0.npy')\n",
        "print(b0.shape)\n",
        "b1=np.load('batch_1.npy')\n",
        "print(b1.shape)\n",
        "b2=np.load('batch_2.npy')\n",
        "print(b2.shape)\n",
        "b3=np.load('batch_3.npy')\n",
        "print(b3.shape)\n",
        "val=np.load('val.npy')\n",
        "print(val.shape)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HEV-5ImNTGjm",
        "outputId": "d81a77f2-0a88-4e28-a1ee-d923ffa296cd"
      },
      "source": [
        "data = np.concatenate((b0,b1,b2,b3,val), axis=0)\n",
        "del b0\n",
        "del b1\n",
        "del b2\n",
        "del b3\n",
        "del val\n",
        "\n",
        "data=data[:,:-2]\n",
        "print(data.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1HYlGBSwQ5Mi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7941e3b7-e537-4976-f04d-96e731d40ce0"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler as Scaler\n",
        "from sklearn.metrics import mean_absolute_error,mean_squared_error,mean_squared_log_error\n",
        "from sklearn.externals import joblib\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "from keras.utils import plot_model\n",
        "from IPython.display import Image\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "furlocKXTC4p"
      },
      "source": [
        "def saveModel(name,model,scaler):\n",
        "  model_json = model.to_json()\n",
        "  with open(os.path.join(modelDir,name+\".json\"), \"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "\n",
        "  model.save_weights(os.path.join(modelDir,name+\".h5\"))\n",
        "  \n",
        "  joblib.dump(scaler, os.path.join(modelDir,name+\"_scaler\"))\n",
        "\n",
        "  print(\"Saved model to disk\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_z-A38UWTHvg"
      },
      "source": [
        "def loadModel(name):\n",
        "  json_file = open(os.path.join(modelDir,name+\".json\"), 'r')\n",
        "  loaded_model_json = json_file.read()\n",
        "  json_file.close()\n",
        "  model = keras.models.model_from_json(loaded_model_json)\n",
        "  \n",
        "  model.load_weights(os.path.join(modelDir,name+\".h5\"))\n",
        "\n",
        "  scaler=joblib.load(os.path.join(modelDir,name+\"_scaler\"))\n",
        "  \n",
        "  print(\"Loaded model from disk\")\n",
        "\n",
        "  return model,scaler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sg7FXEls84Mv"
      },
      "source": [
        "def split_X_Y(data):\n",
        "  #return data[:,:-3],data[:,-3:]\n",
        "  return data[:,:-1],data[:,-1].reshape(-1,1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJHYyGtDWv6N"
      },
      "source": [
        "validationRatio=0.2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hwMBYHuGTpxR",
        "outputId": "c405c01c-5f09-4f50-98ba-faeeb5834125"
      },
      "source": [
        "def split_train_val(data, val_ratio=validationRatio):\n",
        "  shuffled_indices = np.random.permutation(len(data))\n",
        "  val_set_size = int(len(data) * val_ratio)\n",
        "  val_indices = shuffled_indices[:val_set_size]\n",
        "  train_indices = shuffled_indices[val_set_size:]\n",
        "  return data[train_indices], data[val_indices]\n",
        "\n",
        "train,val=split_train_val(data)\n",
        "del data\n",
        "print(train.shape,val.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9XOi_xdKTmTI"
      },
      "source": [
        "scaler = Scaler()\n",
        "train = scaler.fit_transform(train)\n",
        "val = scaler.transform(val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GGt-OB4tTrln",
        "outputId": "2886edfb-3d3a-48d9-d5a2-4dab4ecca2b5"
      },
      "source": [
        "train_x,train_y=split_X_Y(train)\n",
        "del train\n",
        "print(train_x.shape,train_y.shape)\n",
        "\n",
        "val_x,val_y=split_X_Y(val)\n",
        "del val\n",
        "print(val_x.shape,val_y.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vymGiwMZTxtO"
      },
      "source": [
        "def getModel3(normalFeatures, textFeatures, visualFeatures, outputShape):\n",
        "    input = keras.Input(shape=(normalFeatures + textFeatures +\n",
        "                               visualFeatures, ))\n",
        "\n",
        "    normalInput = input[:, :normalFeatures]\n",
        "\n",
        "    textInput = input[:, normalFeatures:textFeatures + normalFeatures]\n",
        "\n",
        "    visualFeatures = input[:, textFeatures + normalFeatures:]\n",
        "\n",
        "    visualFeatures = keras.layers.Dropout(0.5, name='Visual_features_dropout')(visualFeatures)\n",
        "\n",
        "    textInputReshaped = keras.layers.Dense(\n",
        "        300,\n",
        "        activation='elu',\n",
        "        name=\"reshaping_text\",\n",
        "        kernel_regularizer=keras.regularizers.l2(0.01))(\n",
        "            textInput)  ############\n",
        "\n",
        "    visualInputReshaped = keras.layers.Dense(\n",
        "        300,\n",
        "        activation='elu',\n",
        "        name=\"reshaping_visual\",\n",
        "        kernel_regularizer=keras.regularizers.l2(0.01))(\n",
        "            visualFeatures)  ###########\n",
        "\n",
        "    ### At-fusion#############################\n",
        "    tanh_layer = keras.layers.Dense(\n",
        "        300,\n",
        "        use_bias=False,\n",
        "        activation='tanh',\n",
        "        name=\"at_fusion_tanh\",\n",
        "        kernel_regularizer=keras.regularizers.l2(0.01))  ############\n",
        "\n",
        "    text1 = tanh_layer(textInputReshaped)\n",
        "\n",
        "    visual1 = tanh_layer(visualInputReshaped)\n",
        "\n",
        "    linear_layer = keras.layers.Dense(\n",
        "        1,\n",
        "        use_bias=False,\n",
        "        activation='linear',\n",
        "        name=\"at_fusion_linear\",\n",
        "        kernel_regularizer=keras.regularizers.l2(0.01))  ############\n",
        "\n",
        "    text2 = linear_layer(text1)\n",
        "\n",
        "    visual2 = linear_layer(visual1)\n",
        "\n",
        "    merge = keras.layers.Concatenate(axis=1)([text2, visual2])\n",
        "\n",
        "    alpha_fuse = keras.layers.Dense(2,\n",
        "                                    activation='softmax',\n",
        "                                    name=\"at_fusion_softmax\")(merge)\n",
        "\n",
        "    alpha1_expanded = tf.expand_dims(alpha_fuse[:, 0], 1)\n",
        "\n",
        "    text3 = keras.layers.multiply([textInputReshaped, alpha1_expanded])\n",
        "\n",
        "    alpha2_expanded = tf.expand_dims(alpha_fuse[:, 0], 1)\n",
        "\n",
        "    visual3 = keras.layers.multiply([visualInputReshaped, alpha2_expanded])\n",
        "\n",
        "    F = keras.layers.Add()([text3, visual3])\n",
        "    ###########################################\n",
        "\n",
        "    F2 = tf.keras.layers.Concatenate(axis=1)([normalInput, F])\n",
        "\n",
        "    F2 = keras.layers.BatchNormalization()(F2)\n",
        "\n",
        "    F2 = keras.layers.Dropout(0.5)(F2)\n",
        "    hidden = keras.layers.Dense(\n",
        "        308,\n",
        "        activation='elu',\n",
        "        name=\"hidden1\",\n",
        "        kernel_regularizer=keras.regularizers.l2(0.01))(F2)  ##########\n",
        "\n",
        "    hidden = keras.layers.Dropout(0.5)(hidden)\n",
        "\n",
        "    hidden = keras.layers.Dense(\n",
        "        308,\n",
        "        activation='elu',\n",
        "        name=\"hidden2\",\n",
        "        kernel_regularizer=keras.regularizers.l2(0.01))(hidden)  #########\n",
        "\n",
        "    output = keras.layers.Dense(outputShape,\n",
        "                                activation='linear',\n",
        "                                name=\"linear\")(hidden)\n",
        "\n",
        "    model = keras.Model(input, output)\n",
        "\n",
        "    return model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EL1lVcreWR-b"
      },
      "source": [
        "def getModel2(inputShape,outputShape):\n",
        "  model = keras.Sequential([\n",
        "      keras.layers.Dense(1000, kernel_regularizer=keras.regularizers.l2(0.01),\n",
        "                  activation='relu', input_shape=(inputShape,)),\n",
        "      keras.layers.Dropout(0.5),\n",
        "      keras.layers.Dense(1000, kernel_regularizer=keras.regularizers.l2(0.01),\n",
        "                  activation='relu'),\n",
        "      keras.layers.Dropout(0.4),\n",
        "      keras.layers.Dense(1000, kernel_regularizer=keras.regularizers.l2(0.01),\n",
        "                  activation='relu'),\n",
        "      keras.layers.Dropout(0.4),\n",
        "      keras.layers.Dense(1000, kernel_regularizer=keras.regularizers.l2(0.01),\n",
        "                  activation='relu'),\n",
        "      keras.layers.Dropout(0.4),\n",
        "      keras.layers.Dense(1000, kernel_regularizer=keras.regularizers.l2(0.01),\n",
        "                  activation='relu'),\n",
        "                  \n",
        "      keras.layers.Dense(outputShape, activation='linear')\n",
        "  ])\n",
        "\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DgTWodPRT2sP"
      },
      "source": [
        "model = getModel3(8,100,25088,1)\n",
        "#model = getModel2(train_x.shape[1],train_y.shape[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Du3N6vf2T5I1",
        "outputId": "3957b656-0458-4e14-94c2-e43d91a6824b"
      },
      "source": [
        "model.compile(optimizer='adam', loss='mse',metrics=['mean_squared_error'])\n",
        "print(model.summary())\n",
        "\n",
        "# Plot model graph\n",
        "plot_model(model, show_shapes=True, show_layer_names=True, to_file='model.png')\n",
        "Image(retina=True, filename='model.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AydoAlp5T6tM"
      },
      "source": [
        "model_name=\"model\"\n",
        "model.fit(train_x, train_y,\n",
        "                epochs=200,\n",
        "                batch_size=64,\n",
        "                shuffle=True,\n",
        "                validation_data=(val_x,val_y)\n",
        "                )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ySIFw8gNT_Q0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37120fc5-717c-4d83-d0bc-d4da0258d938"
      },
      "source": [
        "saveModel(model_name,model,scaler)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oScpejd6U5lC"
      },
      "source": [
        "def testModel(name=None):\n",
        "\n",
        "  test=val=np.load('/content/dataset/test.npy')\n",
        "  test=test[:,:-2]\n",
        "\n",
        "  test2=scaler.transform(test)\n",
        "\n",
        "  test_x,test_y=split_X_Y(test2)\n",
        "\n",
        "  yhat = model.predict(test_x)\n",
        "\n",
        "  mae = mean_absolute_error(test_y, yhat)\n",
        "  print('MAE: %.5f' % mae)\n",
        "  mse = mean_squared_error(test_y, yhat)\n",
        "  print('MSE: %.5f' % mse)\n",
        "\n",
        "  print(model.evaluate(test_x,test_y))\n",
        "\n",
        "  if name==None:\n",
        "    return\n",
        "\n",
        "  test2[:,-3:]=yhat\n",
        "  test2=scaler.inverse_transform(test2)\n",
        "  \n",
        "  fileAns=name+'_comparison.csv'\n",
        "\n",
        "  with open(fileAns,'w') as fp:\n",
        "    #fp.write('reaction,comment_sentiment,comment_count,predicted_reaction,predicted_comment_sentiment,predicted_comment_count\\n')\n",
        "    fp.write('reactions,predicted_reactions\\n')\n",
        "    for i in range(len(test)):\n",
        "      st=''\n",
        "      for j in range(-1,0):\n",
        "        st+=str(test[i][j])+','\n",
        "\n",
        "      for j in range(-1,0):\n",
        "        st+=str(test2[i][j])+','\n",
        "      \n",
        "      fp.write(st[:-1]+'\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LO7n9wnbVNvJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66e90d29-7229-47e6-bcf9-7015f3bd3fa6"
      },
      "source": [
        "testModel('model')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uMOpZhLrh9Pm"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}