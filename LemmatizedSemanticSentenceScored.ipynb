{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LemmatizedSemanticSentenceWithoutStemming.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NfYuZDq5ujP8",
        "outputId": "e0f39e3b-4f7b-402d-94e6-43d858eca613"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('vader_lexicon')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L1njQIeUtM0h",
        "outputId": "ccec2977-2464-4049-bf2c-c281f2749088"
      },
      "source": [
        "import time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "import io\n",
        "import unicodedata\n",
        "import numpy as np\n",
        "import re\n",
        "import string\n",
        "from numpy import linalg\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.tokenize import PunktSentenceTokenizer\n",
        "from nltk.tokenize import PunktSentenceTokenizer\n",
        "from nltk.corpus import webtext\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "  \n",
        "with open('SentenceScoredLem.txt', mode = 'x' ) as sst:\n",
        "    with open('mergeData.txt') as f:\n",
        "        lines = f.readlines()\n",
        "        # text = \"Congratulations $ Congratulations! $ Congratulations $ A rit no msg avyo 6 $\"\n",
        "        lcount = 0\n",
        "        for text in lines:\n",
        "            TXT = text\n",
        "            lcount += 1\n",
        "            # print(text)\n",
        "            # sent_tokenizer = PunktSentenceTokenizer(text)\n",
        "            # sents = sent_tokenizer.tokenize(text)\n",
        "            \n",
        "            # print(word_tokenize(text))\n",
        "            # print(sent_tokenize(text))\n",
        "            \n",
        "            # porter_stemmer = PorterStemmer()\n",
        "            \n",
        "            # nltk_tokens = nltk.word_tokenize(text)\n",
        "            \n",
        "            # for w in nltk_tokens:\n",
        "            #     print (\"Actual: % s Stem: % s\" % (w, porter_stemmer.stem(w)))\n",
        "                \n",
        "            \n",
        "            wordnet_lemmatizer = WordNetLemmatizer()\n",
        "            nltk_tokens = nltk.word_tokenize(text)\n",
        "            \n",
        "            lemmatized_nltk_tokens = []\n",
        "            for w in nltk_tokens:\n",
        "                # print (\"Actual: % s Lemma: % s\" % (w, wordnet_lemmatizer.lemmatize(w)))\n",
        "                lemmatized_nltk_tokens.append(wordnet_lemmatizer.lemmatize(w))\n",
        "\n",
        "\n",
        "            # text = nltk.word_tokenize(text)\n",
        "            # print(nltk.pos_tag(text))\n",
        "            lem_sent = \"\"\n",
        "            for w in lemmatized_nltk_tokens:\n",
        "                lem_sent += \" \" + w  \n",
        "            \n",
        "            sid = SentimentIntensityAnalyzer() \n",
        "            # tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
        "            \n",
        "            text = TXT\n",
        "            scores = sid.polarity_scores(lem_sent)\n",
        "            for key in sorted(scores):\n",
        "                sst.write('{0}, '.format(scores[key]))\n",
        "\n",
        "            sst.write(\"\\n\")\n",
        "            if lcount % 1000 == 0: print(lcount)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1000\n",
            "2000\n",
            "3000\n",
            "4000\n",
            "5000\n",
            "6000\n",
            "7000\n",
            "8000\n",
            "9000\n",
            "10000\n",
            "11000\n",
            "12000\n",
            "13000\n",
            "14000\n",
            "15000\n",
            "16000\n",
            "17000\n",
            "18000\n",
            "19000\n",
            "20000\n",
            "21000\n",
            "22000\n",
            "23000\n",
            "24000\n",
            "25000\n",
            "26000\n",
            "27000\n",
            "28000\n",
            "29000\n",
            "30000\n",
            "31000\n",
            "32000\n",
            "33000\n",
            "34000\n",
            "35000\n",
            "36000\n",
            "37000\n",
            "38000\n",
            "39000\n",
            "40000\n",
            "41000\n",
            "42000\n",
            "43000\n",
            "44000\n",
            "45000\n",
            "46000\n",
            "47000\n",
            "48000\n",
            "49000\n",
            "50000\n",
            "51000\n",
            "52000\n",
            "53000\n",
            "54000\n",
            "55000\n",
            "56000\n",
            "57000\n",
            "58000\n",
            "59000\n",
            "60000\n",
            "61000\n",
            "62000\n",
            "63000\n",
            "64000\n",
            "65000\n",
            "66000\n",
            "67000\n",
            "68000\n",
            "69000\n",
            "70000\n",
            "71000\n",
            "72000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2EWLH3fJ-4CB",
        "outputId": "e6910575-3469-4ff7-b28d-dccb9d69ce98"
      },
      "source": [
        "!pip install afinn"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting afinn\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/86/e5/ffbb7ee3cca21ac6d310ac01944fb163c20030b45bda25421d725d8a859a/afinn-0.1.tar.gz (52kB)\n",
            "\r\u001b[K     |██████▎                         | 10kB 16.1MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 20kB 18.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 30kB 21.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 40kB 24.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 51kB 26.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 61kB 6.3MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: afinn\n",
            "  Building wheel for afinn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for afinn: filename=afinn-0.1-cp37-none-any.whl size=53451 sha256=4f2126848e1f1d2dfad5284d2b90df99adfa69ab92de9c15366b2400fd0062a0\n",
            "  Stored in directory: /root/.cache/pip/wheels/b5/1c/de/428301f3333ca509dcf20ff358690eb23a1388fbcbbde008b2\n",
            "Successfully built afinn\n",
            "Installing collected packages: afinn\n",
            "Successfully installed afinn-0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YFillsWW_d06",
        "outputId": "40f2d9fa-64cf-4982-fc98-649b19e63eef"
      },
      "source": [
        "#importing necessary libraries\n",
        "from afinn import Afinn\n",
        "import pandas as pd\n",
        "  \n",
        "#instantiate afinn\n",
        "afn = Afinn()\n",
        "  \n",
        "#creating list sentences\n",
        "# news_df = ['Congratulations $ Congratulations! $ Congratulations $ A rit no msg avyo 6 $e']\n",
        "           \n",
        "# # compute scores (polarity) and labels\n",
        "# scores = [afn.score(article) for article in news_df]\n",
        "# sentiment = ['positive' if score > 0 \n",
        "#                           else 'negative' if score < 0 \n",
        "#                               else 'neutral' \n",
        "#                                   for score in scores]\n",
        "      \n",
        "# afn.score('Congratulations $ Congratulations! $ Congratulations $ A rit no msg avyo 6 $e ')\n",
        "with open('SentenceScoredAFNLem.txt', mode = 'x' ) as ast:\n",
        "    with open('mergeData.txt') as f:\n",
        "        lines = f.readlines()\n",
        "        # text = \"Congratulations $ Congratulations! $ Congratulations $ A rit no msg avyo 6 $\"\n",
        "        lcount = 0\n",
        "        for text in lines:\n",
        "            TXT = text\n",
        "            lcount += 1\n",
        "            # print(text)\n",
        "            # sent_tokenizer = PunktSentenceTokenizer(text)\n",
        "            # sents = sent_tokenizer.tokenize(text)\n",
        "            \n",
        "            # print(word_tokenize(text))\n",
        "            # print(sent_tokenize(text))\n",
        "            \n",
        "            # porter_stemmer = PorterStemmer()\n",
        "            \n",
        "            # nltk_tokens = nltk.word_tokenize(text)\n",
        "            \n",
        "            # for w in nltk_tokens:\n",
        "            #     print (\"Actual: % s Stem: % s\" % (w, porter_stemmer.stem(w)))\n",
        "                \n",
        "            \n",
        "            wordnet_lemmatizer = WordNetLemmatizer()\n",
        "            nltk_tokens = nltk.word_tokenize(text)\n",
        "            \n",
        "            lemmatized_nltk_tokens = []\n",
        "            for w in nltk_tokens:\n",
        "                # print (\"Actual: % s Lemma: % s\" % (w, wordnet_lemmatizer.lemmatize(w)))\n",
        "                lemmatized_nltk_tokens.append(wordnet_lemmatizer.lemmatize(w))\n",
        "\n",
        "\n",
        "            # text = nltk.word_tokenize(text)\n",
        "            # print(nltk.pos_tag(text))\n",
        "            lem_sent = \"\"\n",
        "            for w in lemmatized_nltk_tokens:\n",
        "                lem_sent += \" \" + w  \n",
        "            \n",
        "            sid = SentimentIntensityAnalyzer() \n",
        "            # tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
        "            \n",
        "            text = TXT\n",
        "            # scores = sid.polarity_scores(lem_sent)\n",
        "            score = afn.score(lem_sent)\n",
        "            ast.write(str(score))\n",
        "\n",
        "            ast.write(\"\\n\")\n",
        "            if lcount % 1000 == 0: print(lcount)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1000\n",
            "2000\n",
            "3000\n",
            "4000\n",
            "5000\n",
            "6000\n",
            "7000\n",
            "8000\n",
            "9000\n",
            "10000\n",
            "11000\n",
            "12000\n",
            "13000\n",
            "14000\n",
            "15000\n",
            "16000\n",
            "17000\n",
            "18000\n",
            "19000\n",
            "20000\n",
            "21000\n",
            "22000\n",
            "23000\n",
            "24000\n",
            "25000\n",
            "26000\n",
            "27000\n",
            "28000\n",
            "29000\n",
            "30000\n",
            "31000\n",
            "32000\n",
            "33000\n",
            "34000\n",
            "35000\n",
            "36000\n",
            "37000\n",
            "38000\n",
            "39000\n",
            "40000\n",
            "41000\n",
            "42000\n",
            "43000\n",
            "44000\n",
            "45000\n",
            "46000\n",
            "47000\n",
            "48000\n",
            "49000\n",
            "50000\n",
            "51000\n",
            "52000\n",
            "53000\n",
            "54000\n",
            "55000\n",
            "56000\n",
            "57000\n",
            "58000\n",
            "59000\n",
            "60000\n",
            "61000\n",
            "62000\n",
            "63000\n",
            "64000\n",
            "65000\n",
            "66000\n",
            "67000\n",
            "68000\n",
            "69000\n",
            "70000\n",
            "71000\n",
            "72000\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}